{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from models import Model, Trainer\n",
    "from env_test import Env\n",
    "from mcts import MCTS\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "model =Model()# LSTMTagger()#Model()\n",
    "env = Env(1,1)\n",
    "\n",
    "args = dotdict({'cpuct':0.5, 'iters':1000})\n",
    "mcts2 = MCTS(env, model, args)\n",
    "#val = list(mcts2.sampling())\n",
    "#print(len(val))\n",
    "#val = [v for v in val if v.parent and v.parent.times_visited > 2]\n",
    "#print(len(val))\n",
    "t = Trainer(env, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_batch(nodes_buc, batch_size=20):\n",
    "    \"\"\"remove all nodes that have not history\"\"\"\n",
    "    n = len(nodes_buc)\n",
    "    index = [\n",
    "        np.random.randint(0, n - 1)\n",
    "        for _ in range(batch_size)\n",
    "    ]\n",
    "    #print(len(self.replay), index, [self.replay[i] for i in index]   )\n",
    "    return index#[nodes_buc[i] for i in index]\n",
    "\n",
    "\n",
    "def train_model(Xa,yr,yp, model, batch_size=10, net_iters=200):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)#, momentum=0.03, weight_decay=0.1)\n",
    "    loss_backet = []\n",
    "    for i in range(net_iters):\n",
    "        ind = get_batch(Xa)\n",
    "\n",
    "        X, real_reward, real_prob = Xa[ind], yr[ind], yp[ind]#self.transform_bach_as_input(batch, model)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #print(X)\n",
    "        p_pred, v_pred = model(X)\n",
    "        # print('pr  ', probability, 'pp  ', p_pred)\n",
    "        val_loss = torch.mean((Variable(Tensor(real_reward)) - v_pred) ** 2)  # , Variable(Tensor([10]))\n",
    "        loss = val_loss - torch.mean(Variable(Tensor(real_prob)) * torch.log(p_pred))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_backet.append(loss.data.numpy())\n",
    "        \n",
    "    return loss_backet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} 1\n",
      "{} 1799\n",
      "new env 0\n",
      "{1: [2, 64]} 3417\n",
      "{1: [2, 64]} 3862\n",
      "{1: [2, 64]} 4137\n",
      "{1: [2, 64]} 4359\n",
      "{1: [2, 64]} 4515\n",
      "{1: [2, 64]} 4625\n",
      "{1: [2, 64]} 4787\n",
      "{1: [2, 64]} 4887\n",
      "{1: [2, 64]} 4982\n",
      "{1: [2, 64]} 5061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [2, 64]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_linear_out(samples):\n",
    "    X, yv, yp, cresult = t.transform_bach_as_input(samples, model)\n",
    "    return X, yv, yp\n",
    "\n",
    "def env_high_dificult(Env, agent, Trainer):\n",
    "    \n",
    "    epoch = iter_lim = 10\n",
    "    stat = {}\n",
    "    ind_dificult = 1\n",
    "    env = Env(1, ind_dificult)\n",
    "    while epoch:\n",
    "        epoch -= 1\n",
    "        print(stat, len(mcts2.Nodes))\n",
    "        if agent.sum_reward > 10:\n",
    "            stat[ind_dificult] = [iter_lim - epoch, agent.sum_reward]\n",
    "            agent.sum_reward = 0\n",
    "            ind_dificult += 1\n",
    "            epoch = iter_lim\n",
    "            env = Env(1, ind_dificult)\n",
    "            agent.env = env\n",
    "            print('new env', agent.sum_reward)\n",
    "        \n",
    "        val = list(agent.sampling())\n",
    "        val = Trainer.clean_unpredict_node(val)\n",
    "        X, yv, yp = get_linear_out(val)\n",
    "        lossb = train_model(X, yv, yp, agent.model, net_iters=500)\n",
    "        #plt.plot(lossb)\n",
    "        #plt.show()\n",
    "        \n",
    "    return stat\n",
    "\n",
    "mcts2 = MCTS(env, model, args)\n",
    "env_high_dificult(Env, mcts2, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralActionModel(nn.Module):\n",
    "    \"\"\"\n",
    "        predict general action:\n",
    "          ChoseLetter\n",
    "          SeeDif\n",
    "          CreateObj\n",
    "          EditObj\n",
    "          ExpectObj\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, task, vocab_size, internal_mes_size, args, hidden=64):\n",
    "        super(GeneralActionModel, self).__init__()\n",
    "        self.env = env\n",
    "        mcts = MCTS(env, self, args)\n",
    "        self.task = Variable(Tensor(list(range(vocab_size)) + task))\n",
    "        \n",
    "        self.inp_layer = nn.Linear(self.task.shape[0], hidden)\n",
    "        #self.internal = nn.RNNCell(internal_mes, vocab_size + task_size)\n",
    "        self.internal = nn.Linear(internal_mes_size, self.task.shape[0])\n",
    "        self.act = nn.Linear(hidden, 5)\n",
    "        self.message = nn.Linear(hidden, internal_mes_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mes = self.internal(x)\n",
    "        \n",
    "        x = mes * self.task #/ torch.sqrt(torch.max(1 + torch.sum(mes)))\n",
    "        #print(x, mes * self.task, torch.sqrt(1 + torch.sum(mes)))\n",
    "        x = F.relu(self.inp_layer(x))\n",
    "        act_prob = F.softmax(self.act(x))\n",
    "        out_mes = F.tanh(self.message(x))\n",
    "        return act_prob, out_mes\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class ChoseLetter(nn.Module):\n",
    "    def __init__(self, dict_size=8):\n",
    "        super(ChoseLetter, self).__init__()\n",
    "        mcts = ? how add mcts?\n",
    "        env\n",
    "        \n",
    "        \n",
    "    def get_action(self):\n",
    "        return a, sup_vec, back_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1833, 0.1258, 0.2323, 0.2765, 0.1821],\n",
       "         [0.1461, 0.2466, 0.1238, 0.2707, 0.2127]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[ 0.2109,  0.3410, -0.0826, -0.1586, -0.3945, -0.3418, -0.0161,  0.1055,\n",
       "           0.0519, -0.3700, -0.7474, -0.0025,  0.2652, -0.5407,  0.2699,  0.0256,\n",
       "           0.2068, -0.1486, -0.4569, -0.0326, -0.1660,  0.0148, -0.3655,  0.1237,\n",
       "          -0.5987, -0.0374, -0.0978, -0.0145, -0.1224,  0.3118,  0.6009, -0.5222],\n",
       "         [ 0.7098, -0.3471, -0.2454, -0.1384, -0.2364,  0.1239, -0.0915,  0.1675,\n",
       "          -0.1071, -0.3482, -0.8604, -0.6638, -0.2561, -0.6674,  0.5092,  0.0147,\n",
       "           0.3651,  0.0995, -0.5334, -0.3466, -0.6611,  0.4302, -0.7552,  0.6400,\n",
       "          -0.7465,  0.3928,  0.1500, -0.3701, -0.0876,  0.6399, -0.4909, -0.3016]],\n",
       "        grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = dotdict({'cpuct':0.5, 'iters':1000})\n",
    "model = GeneralActionModel(env=Env, task=[1,1], vocab_size=8, internal_mes_size=32, args=args)\n",
    "model(torch.randn([2,32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        models = []\n",
    "        self.actions = range(len(models))\n",
    "    \n",
    "    def do_move(self, a, mes):\n",
    "        return models[a].get_action(mes) # immediate_reward, out_mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
