{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1566885518185,
     "user": {
      "displayName": "Денис Волконский",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCa88QtBa4ugmPsmDvN8iDihIvOioJ15g-G0Qpx=s64",
      "userId": "09425448476864996625"
     },
     "user_tz": -180
    },
    "id": "cSytdP8poSs2",
    "outputId": "50030209-d46e-48eb-f5cc-6acbc35f57c9"
   },
   "outputs": [],
   "source": [
    "# for colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znp4kWzNoS2L"
   },
   "outputs": [],
   "source": [
    "project_dir = '/content/gdrive/My Drive/ColabNotebooks/AI'\n",
    "import sys\n",
    "sys.path.insert(0, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model#, mctsTrainer\n",
    "from env_test import Env\n",
    "from mcts import MCTS\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "model =Model()# LSTMTagger()#Model()\n",
    "env = Env(1,1)\n",
    "\n",
    "args = dotdict({'cpuct':0.5, 'iters':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1918\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mctsTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14011eeb84dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#val = [v for v in val if v.parent and v.parent.times_visited > 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(len(val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmctsTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#examples = deque([], maxlen=1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mctsTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "mcts = MCTS(env, model, args)\n",
    "val = list(mcts.sampling())\n",
    "print(len(val))\n",
    "#val = [v for v in val if v.parent and v.parent.times_visited > 2]\n",
    "#print(len(val))\n",
    "t = mctsTrainer(env, mcts, batch_size=50)\n",
    "t.train_model(val, model, net_iters=300)\n",
    "#examples = deque([], maxlen=1000)\n",
    "\n",
    "batch = t.get_batch(val, batch_size=5)\n",
    "#for b in t.transform_bach_as_input(batch):\n",
    "#    print(b)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(t.loss_backet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeding actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "treegramm_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "treegramm_count\n",
    "\n",
    "def delta(dict1, dict2):\n",
    "    #print(np.sum(list(dict1.values()), dict1.values())\n",
    "    return np.sum(list(dict1.values())) - np.sum(list(dict2.values()))\n",
    "\n",
    "def betreegrams(val, env, treegramm_count):\n",
    "    results = {}\n",
    "    treegramm_count = defaultdict(lambda: defaultdict(int))\n",
    "    for v in val[:100]:\n",
    "        #print(env.result, v.formula)\n",
    "        env.calc_formula(v.formula)\n",
    "        results[v.formula] = env.result.copy()\n",
    "        if env.result['o'] == env.out: print(v.formula)\n",
    "    #print(results)\n",
    "    for v in val[:100]:\n",
    "        # beegram\n",
    "        for i in range(len(v.formula)-2):\n",
    "            # beegram\n",
    "            #print(i, results[v.formula[:i]], v.formula[:i])\n",
    "            if i > 0 and delta(results[v.formula[:i]], results[v.formula[:i + 1]]) != 0:\n",
    "                \n",
    "                treegramm_count[v.formula[i-1]][v.formula[i]] += 1\n",
    "                treegramm_count[v.formula[i-1:i+1]][v.formula[i+1]] += 1\n",
    "            if i > 0 and delta(results[v.formula[:i]], results[v.formula[:i + 2]]) != 0:\n",
    "                # ?? peace of sheet!! eB1 in 1ABCieB1e we could remove it...\n",
    "                #print('aa', v.formula, v.formula[:i+1], results[v.formula[:i]], results[v.formula[:i+2]],\n",
    "                #      v.formula[i-1], v.formula[i-1:i+2]\n",
    "                #     )\n",
    "                treegramm_count[v.formula[i-1]][v.formula[i]] += 1\n",
    "                treegramm_count[v.formula[i-1:i+1]][v.formula[i+1]] += 1\n",
    "        i = len(v.formula) - 1\n",
    "        if i >= 0 and delta(results[v.formula[:i]], results[v.formula[:i + 1]]) != 0:\n",
    "            treegramm_count[v.formula[i-1]][v.formula[i]] += 1\n",
    "    return treegramm_count, results\n",
    "    \n",
    "            \n",
    "t, r = betreegrams(val, env, treegramm_count)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_grams(val, env, treegramm_count):\n",
    "    results = {}\n",
    "    treegramm_count = defaultdict(lambda: defaultdict(int))\n",
    "    treegramm_list = []\n",
    "    for v in val:\n",
    "        #print(env.result, v.formula)\n",
    "        env.calc_formula(v.formula)\n",
    "        results[v.formula] = env.result.copy()\n",
    "        if env.result['o'] == env.out: print(v.formula)\n",
    "    #print(results)\n",
    "    for v in val:\n",
    "        # beegram\n",
    "        for i in range(len(v.formula)-2):\n",
    "            if i>0 and delta(results[v.formula[:i+1]], results[v.formula[:i+2]]):\n",
    "                #print(v.formula)\n",
    "                #treegramm_count[v.formula[i:i+1]][v.formula[i+1:i+2]] += 1\n",
    "                treegramm_count[v.formula[i-1:i+1]][v.formula[i+1:i+2]] += 1\n",
    "                #treegramm_list.append([list(v.formula[i-1:i+1]), v.formula[i+1:i+2]])\n",
    "                treegramm_list.append([list(v.formula[i-1:i]), v.formula[i:i+1]])\n",
    "                treegramm_list.append([list(v.formula[i:i+1]), v.formula[i+1:i+2]])\n",
    "            # beegram\n",
    "            \n",
    "        #i = len(v.formula) - 1\n",
    "        #if i >= 0 and delta(results[v.formula[:i]], results[v.formula[:i + 1]]) != 0:\n",
    "            #treegramm_count[v.formula[i-1]][v.formula[i]] += 1\n",
    "    return treegramm_count, treegramm_list, results\n",
    "\n",
    "t, tl, r = count_grams(val, env, treegramm_count)   \n",
    "#tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 1\n",
    "EMBEDDING_DIM = 10\n",
    "trigrams = tl#[([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "           # for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(env.action_space)\n",
    "word_to_ix = {word: i for i, word in enumerate(env.action_space)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = np.vstack([model.embeddings(torch.tensor([i])).data.numpy() for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#dataframe = np.concatenate([samples, test], axis=0)#[train, test, samples], axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_feature_reduced = pca.fit(dataframe).transform(dataframe)\n",
    "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "#word_vectors_pca = # YOUR CODE\n",
    "\n",
    "# and maybe MORE OF YOUR CODE here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_feature_reduced[:,0], y=X_feature_reduced[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(X_feature_reduced[:, 0], X_feature_reduced[:, 1], token=list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnv1 = nn.Conv1d(1,6,8)\n",
    "        self.fc1 = nn.Linear(17, 182)\n",
    "        self.fc2 = nn.Linear(182, 40)\n",
    "        self.action_prob_out = nn.Linear(40, 8)\n",
    "        #self.val0 = nn.Linear(40, 80)\n",
    "        self.val = nn.Linear(40, 8)\n",
    "\n",
    "        #self.optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.1, weight_decay=0.1)\n",
    "        #self.loss_backet = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #x = x.view(-1,17)\n",
    "        #x = self.cnv1(x.view(1, 1,-1))\n",
    "        # print(x.shape)\n",
    "        #x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        act_prob = F.softmax(self.action_prob_out(x), dim=-1)\n",
    "        val = F.tanh(self.val(x))\n",
    "        #val_sum = self.val_sum_out(val)\n",
    "\n",
    "        return act_prob, val\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        x = Variable(Tensor(x))\n",
    "        act_prob, val = self.forward(x)\n",
    "        return act_prob.data.numpy(), val.data.numpy()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.15061833, 0.11975823, 0.10892252, 0.13129112, 0.1022567 ,\n",
       "         0.13925916, 0.10693832, 0.14095573]], dtype=float32),\n",
       " array([[-0.06885183,  0.11274086, -0.00779156, -0.16206309,  0.08953172,\n",
       "         -0.05558701,  0.02980357,  0.06103383]], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.MAXL = 25\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.aprob_pred = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.val_pred = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.result_pred = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        slen, b_sz, ind_s = sentence.shape\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(embeds.shape, embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out, hidden = self.lstm(embeds.view(len(sentence), 1, self.embedding_dim))\n",
    "        hidden = hidden[0].view(b_sz, self.hidden_dim)\n",
    "        aprob_pred = F.softmax(self.aprob_pred(hidden), dim=1)\n",
    "        val_pred = self.val_pred(hidden)\n",
    "        result_pred = self.result_pred(hidden)\n",
    "        return aprob_pred, val_pred, result_pred\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        #x = Variable(torch.LongTensor(x))\n",
    "        act_prob, val, _ = self.forward(x)\n",
    "        return act_prob.data.numpy(), val.data.numpy()\n",
    "    \n",
    "    def get_observation(self, formula, env):\n",
    "        seq = np.zeros(self.MAXL).reshape(-1,1,1)\n",
    "        \n",
    "        formula_index = np.array([env.action_space.index(a) for a in formula])#.reshape([len(formula),-1,1])\n",
    "        #print(seq[:1,0,0], formula_index)\n",
    "        seq[:formula_index.shape[0],0,0] = formula_index                         \n",
    "        return Variable(torch.LongTensor(seq))\n",
    "    \n",
    "model = LSTMTagger(4, 32, 8, 8)\n",
    "#X = Variable(torch.LongTensor(np.array([[1,1,1,1], [1,1,1,1]])))\n",
    "model.predict(model.get_observation('ieo', env).reshape([-1,1,1]))\n",
    "#model.get_observation('ieo', env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
       "        8., 8., 8., 8., 8., 8., 8.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(25) + 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mctsTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-20e37303b6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmctsTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sorted_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msort_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mctsTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "t = mctsTrainer(env, mcts, batch_size=20)\n",
    "\n",
    "def get_sorted_batch(val, batch_size=20):\n",
    "    sort_val = sorted(val, key=lambda x: len(x.formula))\n",
    "    i = np.random.randint(len(val) - batch_size)\n",
    "    yield sort_val[i:i + batch_size]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sbach = list(get_sorted_batch(val))\n",
    "#print(sbach[0])\n",
    "#for i in sbach[0]: \n",
    "#    print(i.formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    batch = list(get_sorted_batch(val))\n",
    "    \n",
    "    torch.cat([model.get_observation(n.formula, env) for n in batch]).view(t.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 1\n",
    "EMBEDDING_DIM = 10\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(env.action_space)\n",
    "word_to_ix = {word: i for i, word in enumerate(env.action_space)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data(val):\n",
    "    print(val.formula, val.fin_prob, val.ucb_score())\n",
    "\n",
    "shuffle(val)\n",
    "for i in range(30):\n",
    "    val_data(val[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts.Nodes['i1iBo'].times_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.transform_bach_as_input(t.get_batch(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'1': 1, 'i': 0, 'o': 0, 'A': 0, 'B': 0, 'C': 0}\n",
    "keys=list(values) + ['s', 'e']\n",
    "\n",
    "dd = {'formula':[], 'val_sum':[]}\n",
    "for i in replay_buffer.replay:\n",
    "    dd['formula'].append(i[2])\n",
    "    dd['val_sum'].append(i[3].value_sum)\n",
    "    \n",
    "    for j, k in enumerate(keys):\n",
    "        #print('j', j, i[4])\n",
    "        #print(i, i[4][j])\n",
    "        dd[k] = i[4][j]\n",
    "\n",
    "df = pd.DataFrame(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['formula']=='']#['ucb'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['formula']=='ie']#['ucb'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "F.softmax(Tensor([0,0,0])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHT8MKGkKgMZ"
   },
   "outputs": [],
   "source": [
    "test_env.reset()\n",
    "Vocab, Vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF4aIxxZKgMd"
   },
   "outputs": [],
   "source": [
    "j.select_best_leaf().action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOQXFnS1KgMi"
   },
   "outputs": [],
   "source": [
    "j.select_best_leaf().rollout(env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIrncnZsKgMm"
   },
   "outputs": [],
   "source": [
    "test_env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-cTCUZ1KgMs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NidVAYKCKgMw"
   },
   "outputs": [],
   "source": [
    "test_env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMg5FKlQKgM4"
   },
   "outputs": [],
   "source": [
    "test_env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkCYSESeKgM_"
   },
   "outputs": [],
   "source": [
    "test_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aI3S4cplKgNJ"
   },
   "outputs": [],
   "source": [
    "test_env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0cm_RpRKgNN"
   },
   "outputs": [],
   "source": [
    "test_env.step(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeL1SNw8KgNQ"
   },
   "source": [
    "# Imetation learning \n",
    "### if we already know answer we should show it to mcts tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3OmftFqKgNS",
    "outputId": "ce5b2f36-6702-4124-b61e-3ac00cdc1b79"
   },
   "outputs": [],
   "source": [
    "#text = \"input_eq_A A[2]_eq_out\"\n",
    "def text_to_actions(text, vocab=Vocab):\n",
    "    action_seq = []\n",
    "    i = 0\n",
    "    operator = ''\n",
    "    while i < len(text):\n",
    "        if text[i] in Vocab:\n",
    "            action_seq.append(Vocab.index(text[i]))\n",
    "        else:\n",
    "            operator += text[i]\n",
    "            #print('ss', text[i], operator)\n",
    "            if operator == 'input':\n",
    "                operator = ''\n",
    "                \n",
    "            if operator in Vocab:\n",
    "                action_seq.append(Vocab.index(operator))\n",
    "                operator = ''\n",
    "\n",
    "\n",
    "        i += 1\n",
    "    return action_seq\n",
    "\n",
    "for i in text_to_actions(text):\n",
    "    print(Vocab[i])\n",
    "text_to_actions(text)\n",
    "        #print(Vocab.index(i))\n",
    "        #print(env.step(1))\n",
    "        #print(env.step(Vocab.index(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVYXJY8CKgNX"
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyISzsg9KgNa"
   },
   "outputs": [],
   "source": [
    "#text = \"input_sum_A\" \n",
    "for i in text_to_actions(text):\n",
    "    print(i)\n",
    "    print(Vocab[i])\n",
    "    print(env.step(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fq8wx-GIKgNe",
    "outputId": "c2bc22a2-11d0-47fb-ae78-de64f7916297"
   },
   "outputs": [],
   "source": [
    "text = \"input_eq_A A[2]_const_AA AA_eq_out\" \n",
    "c = CalcNode(text)\n",
    "c.calc(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HvILK6NKgNi",
    "outputId": "b06ec81a-9728-4fb8-e463-764bb2c402d8"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "#import mcts\n",
    "importlib.reload(mcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGE-MtmQKgNm"
   },
   "outputs": [],
   "source": [
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = mcts.Root(root_snapshot,root_observation)\n",
    "\n",
    "#plan_mcts(root, env, n_iters=10000, t_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcktRK9PKgNr",
    "outputId": "8a97b6ff-9d2b-4f4c-f71f-07ca8ba61cb3"
   },
   "outputs": [],
   "source": [
    "root.is_leaf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dobGrIBhKgNw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4tbTs7QKgN1"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    is_done = env.step(env.action_space.sample())[2]\n",
    "    if is_done: \n",
    "        print(\"Whoops! We died!\")\n",
    "        break\n",
    "        \n",
    "print(\"final state:\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCfndGRvKgN7"
   },
   "outputs": [],
   "source": [
    "def teach_mcts(root, env, action_seq):\n",
    "    \"\"\"\n",
    "    builds tree with monte-carlo tree search for n_iters iterations\n",
    "    :param root: tree node to plan from\n",
    "    :param n_iters: how many select-expand-simulate-propagete loops to make\n",
    "    \"\"\"\n",
    "    #node = root\n",
    "    #node = mcts.Node(root, action_seq[0], env)\n",
    "    #root.children.add(node)\n",
    "    for action in action_seq:\n",
    "        #print(action)\n",
    "        #print(node.is_done)\n",
    "        \n",
    "        #next_node = mcts.Node(next_node, action, env)\n",
    "        #node.children.add(mcts.Node(node, action, env))\n",
    "        #node = next_node\n",
    "        \n",
    "        #print(node.is_done)\n",
    "        node = root.select_best_leaf()\n",
    "        \n",
    "        #print(Vocab[node.action])\n",
    "        \n",
    "        if node.is_done:\n",
    "            if node.immediate_reward > 0:\n",
    "                print(\"get_reward mcts plan!\", node.immediate_reward)  \n",
    "            \n",
    "            node.propagate(0)\n",
    "            #env.reset()\n",
    "       \n",
    "        else: #node is not terminal\n",
    "            \n",
    "            #print(_)\n",
    "            #env.reset()\n",
    "            #self.env.close()\n",
    "            #self.render()\n",
    "            node.expand(env)\n",
    "            #print(Vocab[node.action])\n",
    "            for i in node.children:\n",
    "                print('dd', )#Vocab[node.action], node.parent.action)\n",
    "            ## value = node.rollout(env, 10)\n",
    "            #if value >0:\n",
    "            #    print('plan after rolout', value, node.action)\n",
    "            ##node.propagate(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wpJsyD4KgN_",
    "outputId": "b072c7e6-6701-47ce-e7d5-88fb46a0de64"
   },
   "outputs": [],
   "source": [
    "teach_mcts(root, env, text_to_actions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDYbBZD1KgOD"
   },
   "outputs": [],
   "source": [
    "p = mcts.Node(root, 2, env)\n",
    "#p.is_leaf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LmjGfWZKgOI"
   },
   "outputs": [],
   "source": [
    "mcts.plan_mcts(root,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHYw3ZLhKgOa",
    "outputId": "f7f5c650-8cdc-4f57-ceff-5b7b0f12f0eb"
   },
   "outputs": [],
   "source": [
    "for i in root.children:\n",
    "    print(Vocab[i.action], i.value_sum, len(i.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4urPh38KgOi",
    "outputId": "4ecbf4f9-e351-4d1e-9710-bbc1c3e504fc"
   },
   "outputs": [],
   "source": [
    "j = root\n",
    "while j.children:\n",
    "    j = j.children.pop()\n",
    "    print(j.action, len(j.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPmBVHd9KgOm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AI_env.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
